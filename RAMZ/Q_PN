#!/bin/bash

# Tampilkan banner judul secara nyata (bukan komentar)
echo "ğŸŸ£â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ğŸŸ£"
echo "             ğŸš€ Google Cloud Dataflow Setup - By RAMZ DH ğŸ§              "
echo "ğŸŸ£â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ğŸŸ£"

# ğŸ‘‰ Prompt for project ID and bucket name
echo "ğŸ”§ Setting up your Google Cloud Dataflow environment!"
read -p "ğŸ“ Enter your GCP Project ID: " PROJECT_ID
read -p "ğŸª£ Enter a unique GCS Bucket Name (e.g. my-unique-bucket): " BUCKET_NAME

# ğŸŒ Set default region
echo "ğŸŒ Setting compute region to us-west1..."
gcloud config set compute/region us-west1

# â˜ï¸ Create Cloud Storage bucket
echo "ğŸ“¦ Creating Cloud Storage bucket: gs://$BUCKET_NAME"
gsutil mb -l us -b on -c STANDARD gs://$BUCKET_NAME

# ğŸ³ Run Python 3.9 Docker container with Apache Beam
echo "ğŸ³ Launching Docker container with Python 3.9 and Apache Beam..."
docker run -it -e DEVSHELL_PROJECT_ID=$PROJECT_ID --rm python:3.9 /bin/bash -c '
  echo "ğŸ“¦ Installing Apache Beam..." &&
  pip install "apache-beam[gcp]"==2.42.0 &&
  echo "ğŸ“„ Running local example: WordCount" &&
  python -m apache_beam.examples.wordcount --output wordcount-output &&
  echo "ğŸ“ Output files:" &&
  ls wordcount-output* &&
  echo "ğŸ“– Output content:" &&
  cat wordcount-output*
'

# ğŸŒ Set BUCKET env var for remote job
BUCKET=gs://$BUCKET_NAME

# ğŸš€ Run remote Dataflow job using DataflowRunner
echo "ğŸš€ Submitting remote Dataflow job to GCP..."
python -m apache_beam.examples.wordcount \
  --project $PROJECT_ID \
  --runner DataflowRunner \
  --staging_location $BUCKET/staging \
  --temp_location $BUCKET/temp \
  --output $BUCKET/results/output \
  --region us-west1

echo "âœ… Dataflow job submitted successfully!"
echo "ğŸ” You can monitor your job in the GCP Console â†’ Dataflow section."
